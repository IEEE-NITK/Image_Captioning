{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b03c1e",
   "metadata": {},
   "source": [
    "# Image captioning with visual attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c39ca",
   "metadata": {},
   "source": [
    "# Loading all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0fb4c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e297981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18f7523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a5b1c",
   "metadata": {},
   "source": [
    "# Declaring Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28301fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image height according the the model architecture\n",
    "IMG_H = 224\n",
    "# image width according the the model architecture\n",
    "IMG_W = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37df9083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for the encoder \n",
    "BATCH_SIZE_ENC = 50\n",
    "# Batch size for the decoder\n",
    "BATCH_SIZE_DEC = 100\n",
    "# Buffer size\n",
    "BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a652dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dir which contains all the images.\n",
    "IMG_FILE_PATH = \"C:\\\\Users\\\\inba2\\\\Documents\\\\DataSet\\\\30k\\\\flickr30k_images\\\\flickr30k_images\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686535c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt file contains the processed captions\n",
    "CAP_TEXT_PATH = r\"C:\\Users\\inba2\\Documents\\DataSet\\30k\\flickr30k_images\\Flickr30k_cap.txt\"\n",
    "# txt file containing the names of all the images\n",
    "IMG_NAMES_TEXT_PATH = r\"C:\\Users\\inba2\\Documents\\DataSet\\30k\\flickr30k_images\\Flickr30k_IMG.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c5e42",
   "metadata": {},
   "source": [
    "# Utility Functions to load and clean images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28701560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_img_name_vector(images_path: str, ext: str = r\".jpg\") -> list:\n",
    "    \"\"\"\n",
    "    :param ext: extension of the image\n",
    "    :returns the path of all the images in that dir\n",
    "    :param images_path: the path of the dir in which the images are present\n",
    "    \"\"\"\n",
    "    # Creating a list of the all the image paths\n",
    "    images_path_list = glob(images_path + '*.jpg')\n",
    "    # Printing the number of images in the given directory\n",
    "    print(f\"{len(images_path_list)} images found from {images_path}.\")\n",
    "    return images_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab53db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(image_path: str, img_h: str = IMG_H, img_w: str = IMG_W) -> (object, str):\n",
    "    \"\"\"\n",
    "    Returns the numpy image and the image path\n",
    "    :param image_path: the path of the image\n",
    "    EG: 'C:\\\\Users\\\\inba2\\\\Documents\\\\DataSet\\\\8k\\\\Images\\\\Images\\\\1000268201_693b08cb0e.jpg'\n",
    "    :param img_w: the width of image taken into the cnn\n",
    "    :param img_h: the height of image taken into the cnn\n",
    "    :return: image and image_path\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Resizing the image according the the model architecture\n",
    "    img = tf.image.resize(img, (img_h, img_w))\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73dd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    to open the file as read only\n",
    "    :param filename: name of the file\n",
    "    :return: the entire text as a string\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "854ad607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_name_2_path(image_name: str, img_file_path: str = IMG_FILE_PATH, ext: str = r\".jpg\") -> str:\n",
    "    \"\"\"\n",
    "    Converts the name of the image to image path\n",
    "    :param image_name: The name of the image\n",
    "    :param img_file_path: The path where the image is stored\n",
    "    :param ext:The extension of the image default is .jpg\n",
    "    :return: The image path\n",
    "    \"\"\"\n",
    "    image_path = img_file_path + str(image_name) + ext\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d57750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_dataset(txt_path: str, batch_size=BATCH_SIZE_ENC):\n",
    "    \"\"\"\n",
    "    To load the train or test dataset\n",
    "    :param txt_path: The text file which has the names of the training images\n",
    "    :param batch_size: The batch size for processing\n",
    "    :return: image_dataset\n",
    "    \"\"\"\n",
    "    img_name_vector = load_set(txt_path)\n",
    "    img_path_list = map(img_name_2_path, img_name_vector)\n",
    "    encode_train = sorted(img_path_list)\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "    image_dataset = image_dataset.map(load_img, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size)\n",
    "    return image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1fc5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(text_file_path: str) -> set:\n",
    "    \"\"\"\n",
    "    to load a pre-defined list of photo names\n",
    "    returns the names of the images form the set_text_file\n",
    "    :param text_file_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # user defined to load the document\n",
    "    doc = load_doc(text_file_path)\n",
    "    dataset = doc.split('\\n')\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c875f61",
   "metadata": {},
   "source": [
    "# Feature extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fb7bd",
   "metadata": {},
   "source": [
    "## Load Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57205489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\inba2\\\\Documents\\\\DataSet\\\\30k\\\\flickr30k_images\\\\Flickr30k_IMG.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_NAMES_TEXT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ca05a",
   "metadata": {},
   "source": [
    " 31783 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819585f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_dataset = load_img_dataset(IMG_NAMES_TEXT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a50d5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.string)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c31469",
   "metadata": {},
   "source": [
    "## Build the image feature extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ab3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model() -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    returns the cnn model need for feature extraction\n",
    "    :return:Vgg16 without the last layer.\n",
    "    \"\"\"\n",
    "    # load the model\n",
    "    model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "    # re-structure the model\n",
    "    model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c97d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "image_features_extract_model = cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66784dc4",
   "metadata": {},
   "source": [
    "## Extraction of features\n",
    "NOTE: the images are saved in the same dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55e1a205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to extract the features of the images[y/Y]: n\n"
     ]
    }
   ],
   "source": [
    "if str(input(\"Do you want to extract the features of the images[y/Y]: \")).casefold() == 'y':\n",
    "    for img, path in tqdm(image_train_dataset):\n",
    "        batch_features = image_features_extract_model(img)\n",
    "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889778d1",
   "metadata": {},
   "source": [
    "# Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890d399",
   "metadata": {},
   "source": [
    "## Utility Functions to load and clean captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc00576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_cap(caption_txt_path: str, dataset: set) -> dict:\n",
    "    \"\"\"\n",
    "    load clean descriptions into memory\n",
    "    :param caption_txt_path: The path where the clean captions were saved\n",
    "    :param dataset: the img names of the tining or test data set\n",
    "    :return: dict of captions mapped with its curr name\n",
    "    \"\"\"\n",
    "    doc = load_doc(caption_txt_path)\n",
    "    clean_captions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split name and captions\n",
    "        image_name, image_cap = tokens[0], \" \".join(tokens[1:])\n",
    "        # skip the images which are not in the set.\n",
    "        if image_name in dataset:\n",
    "            # creating a list\n",
    "            if image_name not in clean_captions:\n",
    "                clean_captions[image_name] = list()\n",
    "            clean_captions[image_name].append(image_cap)\n",
    "    return clean_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88698c5e",
   "metadata": {},
   "source": [
    "## Load captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec402a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\inba2\\\\Documents\\\\DataSet\\\\30k\\\\flickr30k_images\\\\Flickr30k_IMG.txt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_NAMES_TEXT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "341036a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_names = sorted(load_set(IMG_NAMES_TEXT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5faba374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000092795',\n",
       " '10002456',\n",
       " '1000268201',\n",
       " '1000344755',\n",
       " '1000366164',\n",
       " '1000523639',\n",
       " '1000919630',\n",
       " '10010052',\n",
       " '1001465944',\n",
       " '1001545525']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "078a24c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\inba2\\\\Documents\\\\DataSet\\\\30k\\\\flickr30k_images\\\\Flickr30k_cap.txt'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAP_TEXT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9440be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_cap = load_clean_cap(CAP_TEXT_PATH, train_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "853a95a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31783"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_img_cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a531f",
   "metadata": {},
   "source": [
    "## Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "879f1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(clean_captions: dict) -> int:\n",
    "    \"\"\"\n",
    "    Returns the length of the caption with most words\n",
    "    :param clean_captions: a dictionary of captions\n",
    "    :return: length of the longest caption\n",
    "    \"\"\"\n",
    "    # Converts a dictionary of clean captions and returns a list of captions.\n",
    "    clean_captions_list = [caption.split() for captions in clean_captions.values()\n",
    "                           for caption in captions]\n",
    "    return max(len(caption) for caption in clean_captions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e295a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "MAX_CAP_LEN = max_len(train_img_cap)\n",
    "print(MAX_CAP_LEN)\n",
    "MAX_CAP_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "916bb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(captions_dict: dict, top_k: int = 5000) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Fit a tokenizer given caption descriptions\n",
    "    :param captions_dict: (dict) of clean captions\n",
    "    :param top_k: number of words in vocabulary\n",
    "    :return: tokenizer object\n",
    "    \"\"\"\n",
    "    clean_captions_list = [caption for captions in captions_dict.values()\n",
    "                           for caption in captions]\n",
    "    tokenizer = Tokenizer(num_words=top_k, oov_token=\"<unk>\")\n",
    "    # Map '<pad>' to '0'\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "    tokenizer.fit_on_texts(clean_captions_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb5f7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(train_img_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1703da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16860\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d2762",
   "metadata": {},
   "source": [
    "## Creating training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f47a42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_cap(tokenizer: Tokenizer, captions_dict: dict, pad_len: int = MAX_CAP_LEN) -> dict:\n",
    "    \"\"\"\n",
    "    Tokenizes the captions and\n",
    "    :param pad_len: The maximum caption length in the whole dataset\n",
    "    (should include both train and test dataset)\n",
    "    :param tokenizer: Tokenizer object\n",
    "    :param captions_dict: The dict of train/test cap which have to be tokenized\n",
    "    :return: A dict of tokenized captions\n",
    "    \"\"\"\n",
    "    pad_caps_dict = {img_name: pad_sequences(tokenizer.texts_to_sequences(captions), maxlen=pad_len, padding='post',truncating='post')\n",
    "                     for img_name, captions in captions_dict.items()}\n",
    "    return pad_caps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ab8af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_cap_list(img_names_set: set, tokenizer: Tokenizer, captions_dict) -> (list, list):\n",
    "    \"\"\"\n",
    "    a list of image paths and a list of captions for images with corresponding values\n",
    "    Note: the captions will be tokenized and padded in this function\n",
    "    :param img_names_set: The set on which the processing is done\n",
    "    :param tokenizer: tokenizer\n",
    "    :param captions_dict: clean captions for that set without any tokenization\n",
    "    \"\"\"\n",
    "    tokenized_caps_dict = tokenize_cap(tokenizer, captions_dict)\n",
    "    image_name_list = sorted(img_names_set)\n",
    "    capt_list = [cap for name in image_name_list for cap in tokenized_caps_dict[name]]\n",
    "    img_path_list = [img_name_2_path(name) for name in image_name_list for i in range(len(tokenized_caps_dict[name]))]\n",
    "    return img_path_list, capt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79a36f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(img_path_list: str, cap_list: str) -> object:\n",
    "    \"\"\"\n",
    "    :param img_path_list: The ordered list of img paths with duplication acc to number of captions  \n",
    "    :param cap_list: the padded caption list with the curr order \n",
    "    :return: dataset \n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_path_list, cap_list))\n",
    "    # Use map to load the numpy files in parallel\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(load_npy, [item1, item2], [tf.float32, tf.int32]),\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # Shuffle and batch\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE_DEC).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a415326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy(image_path: str, cap: str) -> (str, str):\n",
    "    \"\"\"\n",
    "    :returns image tensor vector with the image path \n",
    "    :param image_path: \n",
    "    :param cap: \n",
    "    \"\"\"\n",
    "    img_tensor = np.load(image_path.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f5893b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c650d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, caption_train = path_cap_list(train_img_names, tokenizer, train_img_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29ef71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(img_name_train, caption_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d8b94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "num_steps = len(train_img_names) // BATCH_SIZE_DEC  # \n",
    "EPOCHS = 20\n",
    "# Shape from last layer of VGG-16 :(7,7,512)\n",
    "# So, say there are 49 pixel locations now and each pixel is 512 dimensional\n",
    "features_shape = 512\n",
    "attention_features_shape = 49 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c743c87",
   "metadata": {},
   "source": [
    "# RNN DECODER \n",
    "# BAHDANAU ATTENTION (LOCAL ATTENTION) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09ab1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "    score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "725d3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 49, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2edc6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#     self.cudnn = tf.compat.v1.keras.layers.CuDNNLSTM(self.units, \n",
    "#                                         return_sequences=True, \n",
    "#                                         return_state=True, \n",
    "#                                         recurrent_initializer='glorot_uniform')\n",
    "    self.gru = tf.compat.v1.keras.layers.CuDNNGRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, MAX_CAP_LEN, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * MAX_CAP_LEN, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * MAX_CAP_LEN, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d2a90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46bb3a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.cudnn_recurrent.CuDNNGRU at 0x2311a3438b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c391fd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CNN_Encoder at 0x2311a32eee0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3a7b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18104e",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbd3effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2bd7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c54f13",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fffe7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2d7446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['startofseq']] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacbf37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_WT = \"Encoder_train_1_30k.h5\"\n",
    "DEC_WT = \"Decoder_train_1_30k.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c51e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(input(\"Do you hanve weights ? [y/Y] \")).casefold() == y:\n",
    "    encoder.load_weights(input(\"Encoder Weight: \"))\n",
    "    decoder.load_weights(input(\"Decoder Weight: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59239790",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a662722",
   "metadata": {},
   "source": [
    "# Saving Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b48ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"Encoder_train_1_30k.h5\") is False:\n",
    "    encoder.save_weights(\"Encoder_train_1_30k.h5\")\n",
    "    print(\"Encoder saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88463504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"Decoder_train_1_30k.h5\") is False:\n",
    "    decoder.save_weights(\"Decoder_train_1_30k.h5\")\n",
    "    print(\"Decoder saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e14c5",
   "metadata": {},
   "source": [
    "# Caption!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((MAX_CAP_LEN, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_img(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['startofseq']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(MAX_CAP_LEN):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == 'endofseq':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08974337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(np.ceil(len_result/2), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = train_img_names[np.random.randint(0, len(train_img_cap))]\n",
    "img_path = img_name_2_path(image_name)\n",
    "real_captions = train_img_cap[image_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, attention_plot = evaluate(img_path)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "print('Real Captions:')\n",
    "for i, cap in enumerate(real_captions):\n",
    "    print(f\"{i+1}) {cap}\")\n",
    "plot_attention(img_path, result, attention_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b1c75",
   "metadata": {
    "id": "Rprk3HEvZuxb"
   },
   "source": [
    "## Try it on your own images\n",
    "For fun, below we've provided a method you can use to caption your own images with the model we've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for weird results!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa46e58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_path = str(input(\"Enter the image path: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f45a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result, attention_plot = evaluate(img_path)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(img_path, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d3fc5",
   "metadata": {},
   "source": [
    "# ------------------------------------ THANK YOU ------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
